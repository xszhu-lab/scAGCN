{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import time\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import DataFrame\n",
    "# from keras import backend as K\n",
    "from math import log\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from keras.datasets import mnist\n",
    "# from keras.models import Model  #采用通用模型\n",
    "# from keras.layers import Dense, Input  #只用到全连接层\n",
    "# from sklearn.metrics.cluster.supervised import contingency_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from math import log\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "\n",
    "sc.settings.verbosity = 3  # verbosity: errors (0), warnings (1), info (2), hints (3)\n",
    "# sc.logging.print_header()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "start_time1 = time.time()\n",
    "\n",
    "data_names = [\n",
    "    'Kumar',\n",
    "#     'KumarTCC',\n",
    "#     'Wang',\n",
    "#     'Wallrapp',\n",
    "#     'Patel',\n",
    "#     'Haber',\n",
    "#     'Petropoulos',\n",
    "#     'Klein',\n",
    "#     'Han',\n",
    "#     'Grun',\n",
    "#     'Cao',\n",
    "#     'Spallanzani',\n",
    "#     'Zemmour',\n",
    "#     'Sala',\n",
    "#     'Shekhar',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kumar\n"
     ]
    }
   ],
   "source": [
    "def del_zero(df, num=1, ratio=0.4):\n",
    "    '''\n",
    "    剔除单一值列和去掉过多的0值特征列\n",
    "    input:\n",
    "    df:原始数据帧\n",
    "    num:单一值阈值\n",
    "    ratio:零值比例，超过则剔除\n",
    "    retrun:\n",
    "    df:筛选过后的数据帧\n",
    "    '''\n",
    "    feature_list = []\n",
    "    for col in df.columns:\n",
    "        unique_num = len(df.loc[:, col].unique())\n",
    "#         print(unique_num)\n",
    "        zeros_ratio = (df.loc[:, col] == 0).sum() / len(df.loc[:, col])\n",
    "        if unique_num > num and zeros_ratio < 0.4:\n",
    "            feature_list.append(col)\n",
    "    return df[feature_list]\n",
    "\n",
    "\n",
    "def data_standardization(X):\n",
    "    '''\n",
    "    数据标准化函数\n",
    "    return:\n",
    "    X:未经过数据标准化的数据\n",
    "    return:\n",
    "    X:经过数据标准化后的数据\n",
    "    '''\n",
    "    X = np.array(X)\n",
    "    std_model = MinMaxScaler()\n",
    "    std_model.fit(X)\n",
    "    X = std_model.transform(X)\n",
    "    X = pd.DataFrame(X)\n",
    "    for i in range(len(X.columns)):\n",
    "        if len(X.iloc[:, i].unique()) in [label_sze, label_sze+1]:\n",
    "            X.iloc[:, i] *= 50\n",
    "    return X\n",
    "\n",
    "\n",
    "def getEntropy(s, pl_n=6):\n",
    "    '''\n",
    "    分箱去噪\n",
    "    input:\n",
    "    s:一个特征数据，一维\n",
    "    pl_n:箱子的阈值,少于这个阈值则去掉\n",
    "    return:\n",
    "    返回信息熵\n",
    "    '''\n",
    "    prt_ary = 0.0\n",
    "    data_vc = s.value_counts()  #统计每个箱子样本数量\n",
    "    data_vc = data_vc[data_vc > pl_n]  #加入箱子的样本数量小于pl_n就去掉\n",
    "    Sum = data_vc.values.sum()\n",
    "    #计算信息熵\n",
    "    for key, value in zip(data_vc.index, data_vc.values):\n",
    "        prt_ary += np.log2(value / Sum) * (value / Sum)\n",
    "    return -prt_ary  #返回信息熵\n",
    "\n",
    "\n",
    "\n",
    "def points_to_the_noise(df, bins = 20,std_ol=4):\n",
    "    '''\n",
    "    分箱去噪 and 异常值平滑\n",
    "    input:\n",
    "    df:原始数据帧\n",
    "    bins:分箱个数\n",
    "    std_ol:认为异常值的标准差倍数\n",
    "    return:\n",
    "    df:筛选过后或经过异常平滑的数据帧\n",
    "    '''\n",
    "    # 数据异常平滑\n",
    "    data_ol = pd.DataFrame()\n",
    "    for col in df.columns:\n",
    "        ol = np.array(df.loc[:, col])\n",
    "        ol_mean = ol.mean()\n",
    "        ol_std = ol.std()\n",
    "        ol[ol > ol_mean + std_ol * ol_std] = ol_mean + std_ol * ol_std\n",
    "        ol[ol < ol_mean - std_ol * ol_std] = ol_mean - std_ol * ol_std\n",
    "        df[col] = ol\n",
    "    #分箱去噪\n",
    "    feature_list = []\n",
    "    for col in df.columns:  #data是DataFrame格式的数据，行是细胞样本  列是特征\n",
    "        bin_data = pd.cut(df.loc[:, col], bins=bins, labels=False)\n",
    "        comentropy = getEntropy(bin_data)\n",
    "        feature_list.append(comentropy > 0.3)\n",
    "    df = df.loc[:, feature_list]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "#hsnw构图\n",
    "import hnswlib\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "def get_ann(length,dimen):\n",
    "    #向量维度\n",
    "    dim = dimen\n",
    "    num_elements = length\n",
    "    data = X\n",
    "    data_labels = np.arange(num_elements)\n",
    "#     k = int(0.1 * int(X.shape[0]))\n",
    "    k = 4\n",
    "    #print(data_labels)\n",
    "    # 声明索引\n",
    "    p = hnswlib.Index(space = 'cosine', dim = dim) # hnswlib支持的距离有L2距离，向量内积以及cosine相似度\n",
    "    # 初始化index\n",
    "    p.init_index(max_elements = num_elements, ef_construction = 100, M = 16)\n",
    "    # ef: 动态检索链表的大小。ef必须设置的比检索最近邻的个数K大。ef取值范围为k到集合大小之间的任意值。\n",
    "    p.set_ef(int(k))\n",
    "    p.set_num_threads(4)#cpu多线程并行计算时所占用的线程数\n",
    "    \n",
    "    #构建items\n",
    "    p.add_items(X, data_labels)\n",
    "    index_path = 'data.bin'\n",
    "    p.save_index(index_path)\n",
    "    global labels, distances\n",
    "    labels, distances = p.knn_query(data, k = k)   #分别包括k个最近邻结果的标签和与这k个标签的距离。\n",
    "    return labels,distances\n",
    "\n",
    "\n",
    "\n",
    "def creat_adjmatrix(new_labels):\n",
    "    #构建邻接矩阵\n",
    "    n1 = np.zeros((len(new_labels),len(new_labels)))   #获取一个样本*样本的全零数组\n",
    "    #赋值，离中心节点近的点的坐标进行赋值\n",
    "    len1 = len(new_labels[0])    \n",
    "    for i in range(len(new_labels)):\n",
    "        for j in range(0,len1):\n",
    "            a=new_labels[i][j]\n",
    "            n1[i][a] = distances[i][j]\n",
    "    b = np.mat(n1)   #转成矩阵\n",
    "    B = b\n",
    "    return B\n",
    "# B = creat_adjmatrix(new_label)\n",
    "\n",
    "#判断正态分布函数\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def decide_data(df, p_=0.05):\n",
    "    '''\n",
    "    df:数据表，DataFrame格式\n",
    "    p_：判断条件，p越大要求越严格\n",
    "    '''\n",
    "    #创建两个列表，分别保存满足正态分布的特征和不满足正态分布的特征\n",
    "    columns1 = []\n",
    "    columns2 = []\n",
    "#     print(df[1].dtype)\n",
    "    for line in df.columns:\n",
    "        #判断是否数值类型，如果不是数值类型则跳过\n",
    "        if df[line].dtype in [ 'int64', 'float64','float32','int32'] :\n",
    "            #判断是否满足正态分布\n",
    "            if normal_test(df[line], p_=0.05):\n",
    "                columns1.append(line)\n",
    "            else:\n",
    "                columns2.append(line)\n",
    "        else:\n",
    "            columns1.append(line)\n",
    "            columns2.append(line)\n",
    "    return df[columns1], df[columns2]\n",
    "\n",
    "def normal_test(ser, p_=0.05):\n",
    "    '''\n",
    "    这是一个正太分布判断函数\n",
    "    ser:传入的待判断数据，一维数组、列表等\n",
    "    p_：判断条件，p越大要求越严格\n",
    "    '''\n",
    "    #转为DataFrame格式\n",
    "    s = pd.DataFrame(np.array(ser),columns = ['value'])\n",
    "    u = s['value'].mean()  # 计算均值\n",
    "    std = s['value'].std()  # 计算标准差\n",
    "    p = stats.kstest(s['value'], 'norm', (u, std))[-1] #得到p值\n",
    "    #判断是否满足正态分布，满足返回True，否则返回False\n",
    "    if p>p_:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# 定义3σ法则识别异常值函数\n",
    "def three_sigma(df, std_=3):\n",
    "    '''\n",
    "    df：表示传入的DataFrame\n",
    "    std_：表示做多少倍的标准差\n",
    "    '''\n",
    "    #定义列表，ol_list存放异常样本的索引，relu1_list存放上届，relu2_list存放下界\n",
    "    ol_list = []\n",
    "    relu1_list = []\n",
    "    relu2_list = []\n",
    "    #变量所有变量\n",
    "    for line in df.columns:\n",
    "        #判断是否数值类型，如果不是数值类型则跳过，否则进行异常值检测\n",
    "        if df[line].dtype in [ 'int64', 'float64'] :\n",
    "            #获取当前待检测变量\n",
    "            Ser1 = df['{}'.format(line)]\n",
    "            #上届限\n",
    "            relu1 = Ser1.mean()+std_*Ser1.std()\n",
    "            #下届限\n",
    "            relu2 = Ser1.mean()-std_*Ser1.std()\n",
    "            #获取异常数组\n",
    "            rule = (relu2>Ser1) | (relu1< Ser1)\n",
    "            #获取异常样本索引\n",
    "            index = np.arange(Ser1.shape[0])[rule]\n",
    "            #异常标注列\n",
    "            df['{}_ol'.format(line)] = np.array(rule).astype('int64')\n",
    "            #分别添加异常样本索引、上届、下界\n",
    "            outrange = Ser1.iloc[index]\n",
    "            ol_list += index.tolist()\n",
    "            relu1_list.append(relu1)\n",
    "            relu2_list.append(relu2)\n",
    "        else:\n",
    "            continue\n",
    "    return df, ol_list, relu1_list, relu2_list\n",
    "\n",
    "def box_plot(df, box_=1.5):\n",
    "    '''\n",
    "    df：表示传入的DataFrame\n",
    "    box_:表示箱线倍数\n",
    "    \n",
    "    '''\n",
    "    #定义列表，ol_list存放异常样本的索引，Low_list存放上限，Up_list存放下限\n",
    "    ol_list = []\n",
    "    Low_list = []\n",
    "    Up_list = []\n",
    "    #遍历所有的列\n",
    "    for line in df.columns:\n",
    "        #判断是否数值类型，如果不是数值类型则跳过，否则进行异常值检测\n",
    "        if df[line].dtype in [ 'int64', 'float64'] :#and len(df['{}'.format(line)].unique())>1\n",
    "            #拿到当前列\n",
    "            Ser = df['{}'.format(line)]\n",
    "            #计算上下限\n",
    "            Low = Ser.quantile(0.25)-box_*(Ser.quantile(0.75)-Ser.quantile(0.25))\n",
    "            Up = Ser.quantile(0.75)+box_*(Ser.quantile(0.75)-Ser.quantile(0.25))\n",
    "            #获取异常索引\n",
    "            index = (Ser< Low) | (Ser>Up)\n",
    "            \n",
    "            #异常标注列\n",
    "            df['{}_ol'.format(line)] = np.array(index).astype('int64')\n",
    "            rule = np.arange(Ser.shape[0])[index]\n",
    "            #分别添加异常样本索引、上届、下界\n",
    "            ol_list += rule.tolist()\n",
    "            Low_list.append(Low)\n",
    "            Up_list.append(Up)\n",
    "        else:\n",
    "            continue\n",
    "    return df, ol_list, Low_list, Up_list\n",
    "\n",
    "\n",
    "def DBSCAN_model(df, min_samples=5, eps_=1):\n",
    "    '''\n",
    "    这是单变量的DBSCAN异常检测模型\n",
    "    df:传入DataFrame格式待检测表\n",
    "    min_samples:最小成簇样本数\n",
    "    eps_：密度可达距离\n",
    "    '''\n",
    "    #定义一个ol_list，用来存放出现异常的样本索引\n",
    "    ol_list = []\n",
    "    #遍历所有特征列\n",
    "    for line in df.columns:\n",
    "        #判断是否数组类型，如果不是数值类型则跳过，否则进行异常值检测\n",
    "        if df[line].dtype in [ 'int64', 'float64','float32','int32'] :#and len(df['{}'.format(line)].unique())>1\n",
    "            #使用DBSCAN进行异常值检测，传入min_samples和eps_\n",
    "            dbscan = DBSCAN(min_samples=min_samples, eps=eps_)\n",
    "            #标准化当前待检测的特征变量\n",
    "            std_data = scale(np.array(df['{}'.format(line)]).reshape((-1, 1)))\n",
    "            #进行聚类\n",
    "            dbscan.fit(std_data)\n",
    "            #判断聚成的类别数，如果类别数大于1，则表明出现了异常样本\n",
    "            if len(pd.Series(dbscan.labels_).unique())>1:\n",
    "                #在sklearn中，将出现异常值的类标记为-1，所以非-1类的都为正常样本标记为0\n",
    "                dbscan.labels_[dbscan.labels_ != -1] = 0\n",
    "                #出现异常的样本表记为1\n",
    "                dbscan.labels_[dbscan.labels_ == -1] = 1\n",
    "                #标记列命名格式为  原列名_ol\n",
    "                df['{}_ol'.format(line)] = dbscan.labels_\n",
    "            else:\n",
    "                #没有出现异常样本，所有样本都标记为0，标记列命名格式为  原列名_ol\n",
    "                df['{}_ol'.format(line)] = np.zeros(df.shape[0], dtype='int64')\n",
    "            #将当前特征变量出现异常值的样本加入ol_list保存\n",
    "            ol_index = df['{}_ol'.format(line)][df['{}_ol'.format(line)]==1].index.tolist()\n",
    "            ol_list += ol_index\n",
    "        else:\n",
    "            continue\n",
    "    #返回一个DataFrame表和出现异常的样本索引\n",
    "    return df, ol_list \n",
    "\n",
    "\n",
    "def  Many_dimensions_DBSCAN(df, min_samples=5, eps_=1):\n",
    "    '''\n",
    "    这是多变量的DBSCAN异常检测模型\n",
    "    df:传入DataFrame格式待检测表\n",
    "    min_samples:最小成簇样本数\n",
    "    eps_：密度可达距离\n",
    "    '''\n",
    "    #定义一个ol_list，用来存放出现异常的样本索引\n",
    "    ol_list = []\n",
    "    #义一个feature_extraction，用来做特征的选取，将需要做检测的特征放入该list\n",
    "    feature_extraction = []\n",
    "    for line in df.columns:\n",
    "        #判断是否数组类型，如果不是数值类型或者字符类型取值个数小于6则跳过，否则进行异常值检测\n",
    "        if df[line].dtype in [ 'int64', 'float64'] or  len(df[line].unique())<6:\n",
    "            #判断是否字符变量\n",
    "            if df[line].dtype=='object' and len(df[line].unique())<6:\n",
    "                #如果是字符变量，则需要数值化\n",
    "                lbe_model = LabelEncoder()\n",
    "                df[line] = lbe_model.fit_transform(df[line])\n",
    "            #添加当前变量到特征列表里\n",
    "            feature_extraction.append(line)\n",
    "    #选取特征\n",
    "    df = df[feature_extraction]\n",
    "    #如果只有一个特征，则需要将这个变量变成两维并标准化，sklearn要求输入两维\n",
    "    if len(df.shape)<0:\n",
    "        std_data = scale(np.array(df).reshape((-1,1)))\n",
    "        print(\"数据表可用特征少于两维！\")\n",
    "        return None\n",
    "    else:\n",
    "        std_data = scale(np.array(df))\n",
    "    #创建一个降维类，将数据降到两维\n",
    "    pca=PCA(n_components=2)\n",
    "    pca_data=pca.fit_transform(std_data)\n",
    "    #创建一个DBSCAN类\n",
    "    dbscan = DBSCAN(min_samples = min_samples, eps=eps_)\n",
    "    #训练\n",
    "    dbscan.fit(pca_data)\n",
    "    #将降维后的数据转为DataFrame格式\n",
    "    pca_data = pd.DataFrame(pca_data, columns=['y1', 'y2'])\n",
    "    #预测类别\n",
    "    clusters = dbscan.fit_predict(pca_data)\n",
    "    #判断聚成的类别数，如果类别数大于1，则表明出现了异常样本\n",
    "    if len(pd.Series(dbscan.labels_).unique())>1:\n",
    "        #在sklearn中，将出现异常值的类标记为-1，所以非-1类的都为正常样本标记为0\n",
    "        dbscan.labels_[dbscan.labels_ != -1] = 0\n",
    "        dbscan.labels_[dbscan.labels_ == -1] = 1\n",
    "        pca_data['outlier'] = dbscan.labels_\n",
    "    else:\n",
    "        #没有出现异常样本，所有样本都标记为0，标记列命名格式为  原列名_ol\n",
    "        pca_data['outlier'] = np.zeros(df.shape[0], dtype='int64')\n",
    "    ol_index = pca_data['outlier'][pca_data['outlier']==1].index.tolist()\n",
    "    #返回一个DataFrame表和出现异常的样本索引\n",
    "    return pca_data, ol_index \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#传入邻居信息\n",
    "def ol_array(data):\n",
    "\n",
    "    '''\n",
    "    单变量检测\n",
    "    input\n",
    "    data：DataFrame格式的待检测二维表\n",
    "    p_：正态分布判断条件，p越大要求越严格\n",
    "    std_：认为异常点的标准差倍数\n",
    "    box_：表示箱线位置\n",
    "    return\n",
    "    ol_list:异常点索引\n",
    "    '''\n",
    "    data1,data2 = decide_data(data.copy(), p_=0.05)\n",
    "    #调用n倍σ单变量检测函数，传入数据表和σ倍数n\n",
    "    sigma_ol, sigmaol_list, upper_limit, lower_limit = three_sigma(data1.copy(), std_=4)\n",
    "    #去重\n",
    "    sigmaol_list = list(set(sigmaol_list))\n",
    "\n",
    "    #调用n倍σ单变量检测函数，传入数据表和σ倍数n\n",
    "    box_ol, boxol_list, upper_limit, lower_limit = box_plot(data2.copy(), box_=2)\n",
    "    #去重\n",
    "    boxol_list = list(set(boxol_list))\n",
    "\n",
    "\n",
    "\n",
    "    ol_list = list(set(sigmaol_list+boxol_list))\n",
    "    return ol_list\n",
    "\n",
    "\n",
    "#这里的data是一个DataFrame格式的矩阵\n",
    "\n",
    "def similitude(a,b):\n",
    "    '''\n",
    "    求x和y的相似性,余弦\n",
    "    '''\n",
    "    if a.shape[0] == b.shape[0]:\n",
    "        a = a.reshape((1, -1))\n",
    "    a_norm = np.linalg.norm(a)\n",
    "    b_norm = np.linalg.norm(b)\n",
    "    cos = (np.dot(a,b)/(a_norm * b_norm)).astype(float)\n",
    "#     cos = cos.astype(float)\n",
    "    return cos\n",
    "#     return np.sum((x-y)**2)**0.5\n",
    "\n",
    "#距离\n",
    "def diff_k(x_test, x_train, k=5):\n",
    "    '''\n",
    "    计算前k个最近邻点，这里用的是欧式距离\n",
    "    '''\n",
    "    diff = x_test-x_train\n",
    "    squreDiff = diff**2\n",
    "    squreDist = np.sum(squreDiff,axis=1)\n",
    "    distances = squreDist ** 0.5\n",
    "    nearest = np.argsort(distances) # 对距离排序并返回对应的索引\n",
    "    nearest_index = nearest[:k]\n",
    "    #获得前k个最近邻点的索引并返回\n",
    "    return nearest_index\n",
    "\n",
    "#根据余弦相似度返回索引\n",
    "def simcos_k(a,b,k = 5):\n",
    "    '''\n",
    "    求x和y的相似性,余弦,返回其索引。\n",
    "    '''\n",
    "    if a.shape[0] == b.shape[0]:\n",
    "        a = a.reshape((1, -1))\n",
    "    a_norm = np.linalg.norm(a)\n",
    "    b_norm = np.linalg.norm(b)\n",
    "    cos = np.dot(a,b)/(a_norm * b_norm)\n",
    "#     trainx = trainx.astype(float)\n",
    "    nearset = np.argsort(cos)\n",
    "    nearest_index = nearset[:k]\n",
    "    return nearest_index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#图卷积自编码\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "def loaddata(data_ol1,label):\n",
    "    Features = np.array(data_ol1.T)\n",
    "    Features = torch.Tensor(Features)\n",
    "    label = np.array(label).reshape(len(label),1)\n",
    "    Labels = label\n",
    "    return(Features, Labels)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "class myGAE(torch.nn.Module):\n",
    "    def __init__(self, d_0, d_1):\n",
    "        super(myGAE, self).__init__()\n",
    "\n",
    "        self.gconv1 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(d_0, d_1),\n",
    "            torch.nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.gconv1[0].weight.data = get_weight_initial(d_1, d_0)\n",
    "\n",
    "        \n",
    "    def Agg(self,data_ol,data):\n",
    "        import math\n",
    "        import torch\n",
    "        #df 是做完领域聚合的结果\n",
    "        data = pd.DataFrame(data.numpy())\n",
    "        data_ol = pd.DataFrame(data_ol.numpy())\n",
    "        df = pd.DataFrame()\n",
    "        data_ol1 = pd.DataFrame(np.array(data_ol),columns=['{}'.format(i) for i in range(data_ol.shape[1])])\n",
    "        data_ol2 = data_ol.T\n",
    "        k=4\n",
    "        cell_data = np.array(data_ol1)    #维度124,2000\n",
    "        #遍历所有的样本点\n",
    "        for cell in range(data.shape[1]):     #data.shape = 124,124\n",
    "            #中心点\n",
    "            df['x_{}'.format(cell)] = np.array(data.iloc[:,cell]).reshape(-1)\n",
    "            #循环迭代更新特征\n",
    "            while True:\n",
    "                #x_test表示中心点,x_test表示中心点的那一列，维度（124，1）\n",
    "                x_test = np.array(data.iloc[:,cell]).reshape((-1, 1))\n",
    "                cell_test =np.array(data_ol2.iloc[:,cell]).reshape((-1, 1))  #维度2000，1\n",
    "                #x_train表示待求中心点最近邻数据集，去除中心点的其余所有列（124，123）\n",
    "                x_train = np.array(data.drop(labels = data.columns[cell], axis=1))\n",
    "                x_train1 = np.array(data)\n",
    "                #获得最近邻索引  根据欧氏距离建立索引，排序取前三个，因为k为3\n",
    "                nearest_index = new_labels[cell]  \n",
    "                #获得k个样本点的特征，相似性，维度为（3，124）\n",
    "                x_k = pd.DataFrame(x_train1[:, nearest_index])\n",
    "#                 #获得邻域异常点索引\n",
    "                ol_index = ol_array(x_k.T)\n",
    "                #原始细胞特征  维度为（12,2000）\n",
    "                cell_k = pd.DataFrame(cell_data[nearest_index,:])  #维度12，2000\n",
    "                cell_k = cell_k.T\n",
    "# #                 删除异常值索引\n",
    "                cell_k = cell_k.drop(labels = cell_k.columns[ol_index],axis = 1)\n",
    "                cell_k = cell_k.T\n",
    "                cell_k = np.array(cell_k)\n",
    "                #设定相容类阈值和其他一些参数\n",
    "                d = 0.95\n",
    "                sims = 0\n",
    "                simuv = 0\n",
    "                #循环中心点和最近邻点求相容类\n",
    "                for i in range(x_k.shape[1]):\n",
    "                    share = 0\n",
    "                    count = 0\n",
    "                    simh = 0\n",
    "                    agg =0\n",
    "                    v = i+1\n",
    "                    for j in range(i, x_k.shape[1]):\n",
    "                        cell_j = np.array(cell_k[j,:]).reshape(-1, 1)\n",
    "                        cell_v = np.array(cell_k[v,:]).reshape(-1, 1)\n",
    "                        #求共享邻居\n",
    "                        nearest_index1 = new_labels[i]\n",
    "                        nearest_index2 = new_labels[j]\n",
    "\n",
    "                        if similitude(cell_v,cell_j)>d:\n",
    "                            #定义一：求相容类的个数\n",
    "                            count = count + 1   \n",
    "                            #求两个交集即共享邻居\n",
    "                            share=len(list(set(nearest_index1).intersection(set(nearest_index2))))\n",
    "                            #定义三：引入共享近邻的邻域节点与中心节点相似度\n",
    "                            sims = (2 * share) / (len(set(nearest_index1)) + len(set(nearest_index2)))\n",
    "\n",
    "\n",
    "                    #定义二：通过邻居节点集的信息熵定义节点u的领域相似度为Sim(г(u))。  \n",
    "                    simh += -(count/k * np.log(count/k))\n",
    "                    #定义四：结合以上两个部分的相似性度量，邻域节点与中心节点之间相似性度量定义如下：simuv = a*sims +b*simh  a+b=1\n",
    "                    a,b,z= 0,0,0\n",
    "                    a = 0.5\n",
    "                    b = 0.5\n",
    "                    simuv = a*sims + b*simh\n",
    "#                     print('simuv',simuv)\n",
    "#                     print('sims',sims)\n",
    "#                     print('simh',simh)\n",
    "                    z = (1 - simuv)\n",
    "                    C = data_ol1.values\n",
    "                    D = B*C\n",
    "                    E = pd.DataFrame(D)\n",
    "                    for m in range(0,cell_k.shape[0]-1):\n",
    "                        if(0.3<z<0.8):\n",
    "                             #找邻居的邻居点索引\n",
    "#                             print('z1',z)\n",
    "                            near_index1 = new_labels[cell]           #一阶邻居索引\n",
    "                            near_index2 = new_labels[near_index1]    #二阶邻居索引\n",
    "                            data_ol01 = data_ol1.iloc[near_index1]    #一阶邻居索引所得特征\n",
    "                            data_ol02 = data_ol1.iloc[near_index2[1]]    #二阶邻居索引所得特征\n",
    "                            n0 = np.ones((1,k))                     #生成1*k的邻接矩阵\n",
    "                            n0 = n0 / k\n",
    "                            n01 =pd.DataFrame(n0).to_numpy()\n",
    "                            n02 = np.dot(n01,data_ol02)                  #得到1*k*k*特征数的矩阵   如1*2000这样\n",
    "                            data_ol11 = data_ol1.values                  #data_ol11为数组类型，目的是进行聚合\n",
    "                            n2 = data_ol11[cell] + n02               #得到聚合之后的一行    一个样本和他的特征\n",
    "                            data_ol11[cell] = n2                    #把得到聚合后的加给该中心节点\n",
    "                            c1 = data_ol11\n",
    "                            d1 = B*c1\n",
    "                            e1 = pd.DataFrame(d1)\n",
    "                            if data_ol1.iloc[i,m] !=0:\n",
    "                                    data_ol1.iloc[i,m] = e1.iloc[i,m]\n",
    "                                \n",
    "                        if(z>0.8):\n",
    "#                             print('z2',z)\n",
    "                            if data_ol1.iloc[i,m] !=0:\n",
    "                                data_ol1.iloc[i,m] = E.iloc[i,m]      #一阶聚合\n",
    "                        if(z<0.3):\n",
    "#                             print('z3',z)\n",
    "                            break\n",
    "                    break\n",
    "                break\n",
    "            break\n",
    "\n",
    "\n",
    "        my_array = np.array(data_ol1)\n",
    "        my_tensor = torch.tensor(my_array)\n",
    "        return(my_tensor)\n",
    "\n",
    "    def Encoder(self, Adjacency_Modified, H_0):\n",
    "        H_0 = self.Agg(H_0, Adjacency_Modified)\n",
    "        H_1 = self.gconv1(torch.matmul(Adjacency_Modified, H_0))\n",
    "#         H_2 = self.gconv2(torch.matmul(Adjacency_Modified, H_1))\n",
    "        return H_1\n",
    "\n",
    "    def Graph_Decoder(self, H_1):\n",
    "        graph_re = Graph_Construction(H_1)\n",
    "        Graph_Reconstruction = graph_re.Middle()\n",
    "        return Graph_Reconstruction\n",
    "\n",
    "\n",
    "    def forward(self, Adjacency_Modified, H_0):\n",
    "        Latent_Representation = self.Encoder(Adjacency_Modified, H_0)\n",
    "        Graph_Reconstruction = self.Graph_Decoder(Latent_Representation)\n",
    "        return Graph_Reconstruction, Latent_Representation\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "def cluster(X, y, label_sze):\n",
    "    ari = nmi = 0\n",
    "    for n_clusters in range(2, 2 * label_sze + 1):\n",
    "        clustering = AgglomerativeClustering(n_clusters=n_clusters).fit(X)\n",
    "        ari_ = metrics.adjusted_rand_score(y.reshape((-1)), clustering.labels_)\n",
    "        nmi_ = metrics.adjusted_mutual_info_score(y.reshape((-1)),\n",
    "                                                  clustering.labels_)\n",
    "        if (ari_ + nmi_) > (ari + nmi):\n",
    "            ari, nmi = ari_, nmi_\n",
    "    return ari, nmi\n",
    "\n",
    "\n",
    "\n",
    "    ################################### Calculate the adjacency matrix #########################################################\n",
    "def graph_autoencoder(Features, Labels, Adjacency_Matrix_raw,label):\n",
    "    Features = torch.Tensor(Features)\n",
    "    Adjacency_Matrix = torch.Tensor(Adjacency_Matrix_raw)\n",
    "    label1 = label\n",
    "    ################################################ adjacency convolution ##################################################\n",
    "    convolution_kernel = Convolution_Kernel(Adjacency_Matrix)\n",
    "    Adjacency_Convolution = convolution_kernel.Adjacency_Convolution()\n",
    "    ############################################ Results  Initialization ###################################################\n",
    "    ACC_GAE_total = []\n",
    "    NMI_GAE_total = []\n",
    "    PUR_GAE_total = []\n",
    "    ARI_GAE_total = []\n",
    "\n",
    "    ACC_GAE_total_STD = []\n",
    "    NMI_GAE_total_STD = []\n",
    "    PUR_GAE_total_STD = []\n",
    "    ARI_GAE_total_STD = []\n",
    "\n",
    "    #######################################  Model #########################################################################\n",
    "    bce_loss = torch.nn.BCELoss(size_average=True)\n",
    "    model_GAE = myGAE(Features.shape[1], Hidden_Layer_1)\n",
    "    optimzer = torch.optim.Adam(model_GAE.parameters(), lr=Learning_Rate)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # cel_loss=torch.nn.CrossEntropyLoss()\n",
    "    # input=torch.Tensor([[-0.7715, -0.6205,-0.2562]])\n",
    "    # target = torch.tensor([0])\n",
    "    # output = entroy(input, target)\n",
    "    # print(output)\n",
    "\n",
    "    #######################################  Train and result ################################################################\n",
    "    for epoch in range(Epoch_Num):\n",
    "\n",
    "        Graph_Reconstruction, Latent_Representation = model_GAE(Adjacency_Convolution, Features)\n",
    "        Graph_Reconstruction=torch.unsqueeze(Graph_Reconstruction,0)\n",
    "    #     print(Graph_Reconstruction.view(-1).shape)\n",
    "    #     print(Adjacency_Matrix.view(-1).shape)\n",
    "        loss = bce_loss(Graph_Reconstruction.view(-1), Adjacency_Matrix.view(-1))\n",
    "\n",
    "        optimzer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimzer.step()\n",
    "\n",
    "        Latent_Representation = Latent_Representation.cpu().detach().numpy()\n",
    "\n",
    "        ##################################################### Results  ####################################################\n",
    "\n",
    "\n",
    "        if Clustering and (epoch + 1) % 5 == 0:\n",
    "            print(\"Epoch:{},Loss:{:.4f}\".format(epoch + 1, loss.item()))\n",
    "            ACC_H2 = []\n",
    "            NMI_H2 = []\n",
    "            PUR_H2 = []\n",
    "            ARI_H2 = []\n",
    "    #         kmeans = KMeans(n_clusters=max(np.int_(Labels).flatten()))\n",
    "#             kmeans = KMeans(n_clusters=label_sze)\n",
    "    #         print(type(Latent_Representation))\n",
    "    #         print(type(label))\n",
    "            label = np.array(label1)\n",
    "    #         print(type(label_sze))\n",
    "    #         StandardScaler().fit_transform(label['label’].values.reshape(-1, 1))\n",
    "            ari, nmi = cluster(Latent_Representation, label, label_sze)\n",
    "            ari_max = 0\n",
    "            nmi_max = 0\n",
    "            if(ari>ari_max):\n",
    "                ari_max = ari\n",
    "            if(nmi>nmi_max):\n",
    "                nmi_max = nmi\n",
    "#             for i in range(10):\n",
    "#                 Y_pred_OK = kmeans.fit_predict(Latent_Representation)     \n",
    "#                 Labels_K = np.array(Labels).flatten()\n",
    "#                 ARI = metrics.adjusted_rand_score(Y_pred_OK, Labels_K) \n",
    "#                 AM = clustering_metrics(Y_pred_OK, Labels_K)\n",
    "#                 ACC, NMI, PUR = AM.evaluationClusterModelFromLabel(print_msg=False)\n",
    "#                 ACC_H2.append(ACC)\n",
    "#                 NMI_H2.append(NMI)\n",
    "#                 PUR_H2.append(PUR)\n",
    "#                 ARI_H2.append(ARI)\n",
    "\n",
    "#             ACC_GAE_total.append(100 * np.mean(ACC_H2))\n",
    "#             NMI_GAE_total.append(100 * np.mean(NMI_H2))\n",
    "#             PUR_GAE_total.append(100 * np.mean(PUR_H2))\n",
    "#             ARI_GAE_total.append(100 * np.mean(ARI_H2))\n",
    "#             ACC_GAE_total_STD.append(100 * np.std(ACC_H2))\n",
    "#             NMI_GAE_total_STD.append(100 * np.std(NMI_H2))\n",
    "#             PUR_GAE_total_STD.append(100 * np.std(PUR_H2))\n",
    "#             ARI_GAE_total_STD.append(100 * np.std(ARI_H2))\n",
    "            \n",
    "#             Index_MAX = np.argmax(ari_max)\n",
    "#             ACC_GAE_max = np.float(ACC_GAE_total[Index_MAX])\n",
    "#             NMI_GAE_max = np.float(NMI_GAE_total[Index_MAX])\n",
    "#             PUR_GAE_max = np.float(PUR_GAE_total[Index_MAX])\n",
    "#             ARI_GAE_max = np.float(ARI_GAE_total[Index_MAX])\n",
    "\n",
    "#             print('ACC_H2=', 100 * np.mean(ACC_H2), '\\n', 'ARI_H2=', 100 * np.mean(ARI_H2), '\\n','NMI_H2=', 100 * np.mean(NMI_H2), '\\n', 'PUR_H2=',\n",
    "#                   100 * np.mean(PUR_H2))\n",
    "\n",
    "            np.save(path_result + \"{}.npy\".format(epoch + 1), Latent_Representation)\n",
    "\n",
    "    ###############################################################Clustering  Result ##############################\n",
    "    if Clustering:\n",
    "        Index_MAX = np.argmax(ari_max)\n",
    "#         ACC_GAE_max = np.float(ACC_GAE_total[Index_MAX])\n",
    "#         NMI_GAE_max = np.float(NMI_GAE_total[Index_MAX])\n",
    "#         PUR_GAE_max = np.float(PUR_GAE_total[Index_MAX])\n",
    "#         ARI_GAE_max = np.float(ARI_GAE_total[Index_MAX])\n",
    "\n",
    "#         ACC_STD = np.float(ACC_GAE_total_STD[Index_MAX])\n",
    "#         NMI_STD = np.float(NMI_GAE_total_STD[Index_MAX])\n",
    "#         PUR_STD = np.float(PUR_GAE_total_STD[Index_MAX])\n",
    "#         ARI_STD = np.float(ARI_GAE_total_STD[Index_MAX])\n",
    "        print('ari_max={}' .format(ari_max))\n",
    "        print('nmi_max={}' .format(nmi_max))\n",
    "#         print('ACC_GAE_max={:.2f} +- {:.2f}'.format(ACC_GAE_max, ACC_STD))\n",
    "#         print('NMI_GAE_max={:.2f} +- {:.2f}'.format(NMI_GAE_max, NMI_STD))\n",
    "#         print('PUR_GAE_max={:.2f} +- {:.2f}'.format(PUR_GAE_max, PUR_STD))\n",
    "#         print('ARI_GAE_max={:.2f} +- {:.2f}'.format(ARI_GAE_max, ARI_STD))\n",
    "#         print(\"The incompleteness of the adjacency matrix is {}%\".format(scale * 100))\n",
    "\n",
    "    ########################################################### t- SNE #################################################\n",
    "    if t_SNE:\n",
    "        print(\"Index_Max = {}\".format(Index_MAX))\n",
    "        Latent_Representation_max = np.load(path_result + \"{}.npy\".format((Index_MAX+1) * 5))\n",
    "        Features = np.array(Features)\n",
    "#         print(type(Labels))\n",
    "#         print(Labels.shape)\n",
    "#         print(Labels)\n",
    "#         print(type(Features))\n",
    "#         print(Features.shape)\n",
    "#         print(Features)\n",
    "        plot_embeddings(Latent_Representation_max, Features, Labels)\n",
    "        savefig(r'C:\\Users\\86166\\Desktop\\2022.4.25实验结果\\聚类图\\pca1\\{}PCA111.jpg'.format(name))\n",
    "        plt.show()\n",
    "        plot_embedding(Latent_Representation_max, Features, Labels)\n",
    "        savefig(r'C:\\Users\\86166\\Desktop\\2022.4.25实验结果\\聚类图\\t-sne1\\{}T-SNE111.jpg'.format(name))\n",
    "        plt.show()\n",
    "    ########################################################################################################################\n",
    "#     end_time = time.time()\n",
    "#     ARI_GAE_max = ARI_GAE_max/100\n",
    "#     NMI_GAE_max = NMI_GAE_max/100\n",
    "    end_time = time.time()\n",
    "    print(\"Running time is {}\".format(end_time - start_time))\n",
    "    return (ari_max,nmi_max)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.metrics import silhouette_score \n",
    "import sys\n",
    "sys.path.append('./Data_Process')\n",
    "path_result = \"./Latent_representation/\"\n",
    "# from Models import *\n",
    "from Metrics import *\n",
    "from sklearn import metrics\n",
    "import scipy.io as scio\n",
    "from Data_Process import *\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib.pyplot import plot,savefig\n",
    "# from pandas_datareader import data, wb\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "######################################################### Setting #####################################################\n",
    "Clustering = True\n",
    "t_SNE = True\n",
    "scale = 0\n",
    "########################################## hyper-parameters##############################################################\n",
    "Epoch_Num = 10\n",
    "Learning_Rate = 1e-4\n",
    "\n",
    "Hidden_Layer_1 = 800\n",
    "# Hidden_Layer_2 = 50\n",
    "\n",
    "################################### Load dataset   ######################################################################\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #定义一个DF，记录指标得分\n",
    "    record = pd.DataFrame(\n",
    "        columns=['名称', 'x', 'y', '类别数','层次聚类 ARI','层次聚类 NMI', '运行时间'])\n",
    "    for name in data_names[:]:\n",
    "        print(name)\n",
    "        start = time.perf_counter() \n",
    "        #读取数据\n",
    "        \n",
    "        data = pd.read_csv(r'D:\\scRNA数据集\\data0317\\data20220307\\{}.csv'.format(name)).iloc[:,1:]\n",
    "#         data = pd.read_csv(r'E:\\data20220307\\data20220307\\{}.csv'.format(name)).iloc[:,1:]\n",
    "        data = pd.DataFrame(data)\n",
    "        # #分离特征和标签\n",
    "        X1 = data.drop('label', axis=1)\n",
    "        #label1 = data['label'].values\n",
    "        label=pd.Categorical(data.iloc[:,-1]).codes\n",
    "        \n",
    "         #记录原始维度\n",
    "        y_shape = X1.shape[1]\n",
    "        x_shape = X1.shape[0]\n",
    "        label_sze = len(np.unique(label))\n",
    "        data1 = pd.DataFrame(X1)\n",
    "        data = del_zero(data1)\n",
    "        data_ol1 = points_to_the_noise(data,bins = 20,std_ol=4)\n",
    "        data_ol = data_standardization(data_ol1)\n",
    "#         X = DataFrame.as_matrix(data_ol)\n",
    "        X = data_ol.values\n",
    "        get_ann(X.shape[0],X.shape[1])  \n",
    "        new_labels = labels\n",
    "        data_ol = data_ol.T\n",
    "        data_ol1 = pd.DataFrame(np.array(data_ol),columns=['{}'.format(i) for i in range(data_ol.shape[1])])\n",
    "        data_ol = pd.DataFrame(np.array(data_ol),columns=['cell{}'.format(i) for i in range(data_ol.shape[1])])\n",
    "        B = creat_adjmatrix(new_labels)\n",
    "        data = pd.DataFrame(B)\n",
    "        Adjacency_Matrix_raw = B\n",
    "        Features, Labels = loaddata(data_ol1,label)\n",
    "#         print(Features.is_cuda)\n",
    "#         print(Labels)\n",
    "        ari, nmi = graph_autoencoder(Features, Labels, Adjacency_Matrix_raw,label)\n",
    "        end = time.perf_counter() \n",
    "        print(ari, nmi)\n",
    "        score = [name, x_shape, y_shape, label_sze, ari, nmi, end - start]\n",
    "        record.loc['{}'.format(name)] = score\n",
    "#         record.to_csv('./实验结果2022.04.25层次聚类.csv')\n",
    "        record.to_csv(r'C:/Users/86166/Desktop/2022.4.25实验结果/评估指标/实验结果2022.04.25.csv')\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
